{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b95cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, Literal, List, Set, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = os.listdir('DataSujetos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b22e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTADIOS = ['W', 'N1', 'N2', 'N3']\n",
    "NUM_SUJETOS = 18\n",
    "ADJ_MATRIX_SIZE = 116\n",
    "GLOBAL_SEED = 158151\n",
    "PC = 0.05\n",
    "ZC = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b654d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    \n",
    "    def __init__(self, density: float):\n",
    "        self.density = density\n",
    "        self._data = {\n",
    "            estadio: {\n",
    "                f'suj{i+1}': {\n",
    "                    'zi': np.zeros(ADJ_MATRIX_SIZE),\n",
    "                    'Pi': np.zeros(ADJ_MATRIX_SIZE),\n",
    "                } for i in range(NUM_SUJETOS)\n",
    "            } for estadio in ESTADIOS\n",
    "         }\n",
    "        \n",
    "    def set_experiment_data(self, \n",
    "                            data: Dict[str, Iterable[float]], \n",
    "                            estadio: Literal['W', 'N1', 'N2', 'N3'], \n",
    "                            sujeto: str) -> None:\n",
    "        self._data[estadio][sujeto] = data\n",
    "            \n",
    "    def get_node_classification_data_by_estadio(self, \n",
    "                                                estadio: Literal['W', 'N1', 'N2', 'N3']\n",
    "                                               ) -> Dict[str, Dict[str, int]]:\n",
    "        relevant_data = self._data[estadio]\n",
    "        ans = {}\n",
    "        for sujeto, raw_data in relevant_data.items():\n",
    "            zi_mask = raw_data['zi'] > ZC\n",
    "            Pi_mask = raw_data['Pi'] > PC\n",
    "            num_Provincial_Hubs = np.sum(zi_mask & ~Pi_mask)\n",
    "            num_Hubs = np.sum(zi_mask & Pi_mask)\n",
    "            num_Connector_Nodes = np.sum(~zi_mask & Pi_mask)\n",
    "            num_Provincial_Nodes = np.sum(~zi_mask & ~Pi_mask)\n",
    "            ans[sujeto] = {\n",
    "                'Provincial_Hubs': num_Provincial_Hubs,\n",
    "                'Hubs': num_Hubs,\n",
    "                'Connector_Nodes': num_Connector_Nodes,\n",
    "                'Provincial_Nodes': num_Provincial_Nodes,\n",
    "            }\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26882df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(adj_matrix: np.ndarray, density: float, epsilon: int=2) -> float:\n",
    "    size = adj_matrix.shape\n",
    "    N = size[0]*size[1]\n",
    "    left = adj_matrix.min()\n",
    "    right = adj_matrix.max()\n",
    "    target = math.ceil(N*density)\n",
    "    res = N\n",
    "    cnt = 0\n",
    "    limit = 100\n",
    "    while res!=target:\n",
    "        cnt+=1\n",
    "        if cnt == limit:\n",
    "            print(f'Have not converged in {limit} iterations, returning suboptimal threshold')\n",
    "            return guess\n",
    "        guess = (left+right)/2\n",
    "        res = np.sum(adj_matrix>=guess)\n",
    "        if (res>=target-epsilon) and (res<=target+epsilon):\n",
    "            return guess\n",
    "        elif res<target:\n",
    "            right = guess\n",
    "        else:\n",
    "            left = guess\n",
    "\n",
    "            \n",
    "def extract_sujeto(filename: str) -> str:\n",
    "    return filename.split('.')[0].split('_')[1]\n",
    "\n",
    "\n",
    "def _compute_zi(G: nx.Graph) -> Dict[int, float]:\n",
    "    degrees = G.degree()\n",
    "    k_mean = np.mean([degree for node, degree in degrees])\n",
    "    k_std = np.std([degree for node, degree in degrees])\n",
    "    return {node: (degree-k_mean)/k_std for node, degree in degrees}\n",
    "\n",
    "\n",
    "def _compute_Pi(G: nx.Graph, comms: List[Set[int]]) -> Dict[int, float]:\n",
    "    # We start with the initial value of 1 and iteratively subtract from it according to the definition\n",
    "    Pi = {node: 1 for node in range(ADJ_MATRIX_SIZE)}\n",
    "    degrees = G.degree()\n",
    "    for node in Pi:\n",
    "        ki = degrees[node]\n",
    "        neighbors = set(nx.all_neighbors(G, node))\n",
    "        for C in comms:\n",
    "            kiUj = len(neighbors.intersection(C))\n",
    "            tmp = (kiUj/ki)**2 if ki>0 else 0\n",
    "            Pi[node] -= tmp\n",
    "    return Pi\n",
    "\n",
    "\n",
    "def run_node_experiment(G: nx.Graph) -> Dict[str, Dict[int, float]]:\n",
    "    comms = nx.community.louvain_communities(G, seed=GLOBAL_SEED)\n",
    "    zi = _compute_zi(G)\n",
    "    zi_values = np.array([x for x in zi.values()])\n",
    "    Pi = _compute_Pi(G, comms)\n",
    "    Pi_values = np.array([x for x in Pi.values()])\n",
    "    return {'zi': zi_values, 'Pi': Pi_values}\n",
    "    \n",
    "    \n",
    "def graph_factory(adjmat: np.ndarray) -> nx.Graph:\n",
    "    adjmat -= np.diag(np.diag(adjmat))\n",
    "    thr = find_threshold(adjmat, d)\n",
    "    adjmat = adjmat>=thr\n",
    "    G = nx.from_numpy_array(adjmat)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a93b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with density 0.01\n",
      "Start with density 0.019999999999999997\n",
      "Start with density 0.03\n",
      "Start with density 0.039999999999999994\n",
      "Start with density 0.049999999999999996\n",
      "Have not converged in 100 iterations, returning suboptimal threshold\n",
      "Start with density 0.05999999999999999\n",
      "Start with density 0.06999999999999999\n",
      "Start with density 0.07999999999999999\n",
      "Start with density 0.08999999999999998\n",
      "Start with density 0.09999999999999998\n",
      "Start with density 0.10999999999999997\n",
      "Start with density 0.11999999999999998\n",
      "Start with density 0.12999999999999998\n",
      "Start with density 0.13999999999999999\n",
      "Start with density 0.15\n",
      "CPU times: total: 20.3 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "densities = np.linspace(0.01, 0.15, num=15)\n",
    "experiments = [Experiment(d) for d in densities]\n",
    "avg_graphs = []\n",
    "running_avg_graph = {\n",
    "    estadio: np.zeros((ADJ_MATRIX_SIZE, ADJ_MATRIX_SIZE)) for estadio in ESTADIOS\n",
    "}\n",
    "\n",
    "for i, d in enumerate(densities):\n",
    "    print(f'Start with density {d}')\n",
    "    exp = experiments[i]\n",
    "    for estadio in ESTADIOS:\n",
    "        for f in files:\n",
    "            if not f.startswith(estadio):\n",
    "                continue\n",
    "            sujeto = extract_sujeto(f)\n",
    "            adjmat = pd.read_csv('DataSujetos/'+f, header=None).values\n",
    "            running_avg_graph[estadio] += adjmat/NUM_SUJETOS\n",
    "            G = graph_factory(adjmat)\n",
    "            node_data = run_node_experiment(G)\n",
    "            exp.set_experiment_data(node_data, estadio, sujeto)\n",
    "    avg_graphs.append(running_avg_graph)\n",
    "    running_avg_graph = {\n",
    "        estadio: np.zeros((ADJ_MATRIX_SIZE, ADJ_MATRIX_SIZE)) for estadio in ESTADIOS\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55e76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = experiments[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069e67fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suj1': {'Provincial_Hubs': 5,\n",
       "  'Hubs': 16,\n",
       "  'Connector_Nodes': 37,\n",
       "  'Provincial_Nodes': 58},\n",
       " 'suj2': {'Provincial_Hubs': 3,\n",
       "  'Hubs': 18,\n",
       "  'Connector_Nodes': 47,\n",
       "  'Provincial_Nodes': 48},\n",
       " 'suj3': {'Provincial_Hubs': 7,\n",
       "  'Hubs': 20,\n",
       "  'Connector_Nodes': 30,\n",
       "  'Provincial_Nodes': 59},\n",
       " 'suj4': {'Provincial_Hubs': 0,\n",
       "  'Hubs': 17,\n",
       "  'Connector_Nodes': 57,\n",
       "  'Provincial_Nodes': 42},\n",
       " 'suj5': {'Provincial_Hubs': 1,\n",
       "  'Hubs': 13,\n",
       "  'Connector_Nodes': 52,\n",
       "  'Provincial_Nodes': 50},\n",
       " 'suj6': {'Provincial_Hubs': 5,\n",
       "  'Hubs': 21,\n",
       "  'Connector_Nodes': 35,\n",
       "  'Provincial_Nodes': 55},\n",
       " 'suj7': {'Provincial_Hubs': 0,\n",
       "  'Hubs': 23,\n",
       "  'Connector_Nodes': 50,\n",
       "  'Provincial_Nodes': 43},\n",
       " 'suj8': {'Provincial_Hubs': 2,\n",
       "  'Hubs': 21,\n",
       "  'Connector_Nodes': 34,\n",
       "  'Provincial_Nodes': 59},\n",
       " 'suj9': {'Provincial_Hubs': 2,\n",
       "  'Hubs': 21,\n",
       "  'Connector_Nodes': 38,\n",
       "  'Provincial_Nodes': 55},\n",
       " 'suj10': {'Provincial_Hubs': 5,\n",
       "  'Hubs': 15,\n",
       "  'Connector_Nodes': 33,\n",
       "  'Provincial_Nodes': 63},\n",
       " 'suj11': {'Provincial_Hubs': 0,\n",
       "  'Hubs': 26,\n",
       "  'Connector_Nodes': 49,\n",
       "  'Provincial_Nodes': 41},\n",
       " 'suj12': {'Provincial_Hubs': 2,\n",
       "  'Hubs': 21,\n",
       "  'Connector_Nodes': 42,\n",
       "  'Provincial_Nodes': 51},\n",
       " 'suj13': {'Provincial_Hubs': 0,\n",
       "  'Hubs': 18,\n",
       "  'Connector_Nodes': 36,\n",
       "  'Provincial_Nodes': 62},\n",
       " 'suj14': {'Provincial_Hubs': 1,\n",
       "  'Hubs': 17,\n",
       "  'Connector_Nodes': 39,\n",
       "  'Provincial_Nodes': 59},\n",
       " 'suj15': {'Provincial_Hubs': 1,\n",
       "  'Hubs': 22,\n",
       "  'Connector_Nodes': 46,\n",
       "  'Provincial_Nodes': 47},\n",
       " 'suj16': {'Provincial_Hubs': 0,\n",
       "  'Hubs': 22,\n",
       "  'Connector_Nodes': 42,\n",
       "  'Provincial_Nodes': 52},\n",
       " 'suj17': {'Provincial_Hubs': 3,\n",
       "  'Hubs': 16,\n",
       "  'Connector_Nodes': 25,\n",
       "  'Provincial_Nodes': 72},\n",
       " 'suj18': {'Provincial_Hubs': 3,\n",
       "  'Hubs': 12,\n",
       "  'Connector_Nodes': 51,\n",
       "  'Provincial_Nodes': 50}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.get_node_classification_data_by_estadio('N1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a6e283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 116)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_graphs[0]['W'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71858ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
